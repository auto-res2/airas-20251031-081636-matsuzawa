
LLM Name: o3-2025-04-16
Input:
You are a researcher in machine learning. Based on the instructions below, please generate a simple new research method with minimal modifications to existing approaches.

# Instructions:
- Read the research topic described below:
    Improving Test-Time Adaptation in terms of convergence speed.
- A list of related prior studies is provided. Each entry contains a summary of its title, main contributions, methodologies, results, and limitations:
    {
    "Title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
    "Main Contributions": "This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods that explicitly accounts for computational time constraints. It penalizes slower methods by providing them with fewer samples for adaptation, thereby offering a more realistic assessment of TTA efficacy in real-world scenarios with constant-speed data streams. The research demonstrates that, when inference speed is considered, simple and fast TTA approaches can often outperform more sophisticated but slower state-of-the-art methods. The paper provides a comprehensive experimental analysis of 15 TTA methods on multiple datasets under various scenarios, highlighting the critical importance of developing TTA methods that are both accurate and efficient.",
    "Methodology": "The proposed Realistic TTA evaluation protocol extends the current offline protocol by introducing a 'relative adaptation speed' C(g) for each TTA method g. C(g) is defined as the integer ratio of the data stream speed (r) to the method's adaptation speed (R(g(x))), calculated online as C(g(xt)) = ⌈r/R(g(x))⌉. In this online setting, if C(g) = k, the method is only allowed to adapt to every k-th sample. Samples skipped by the TTA method (when it's busy adapting) are processed by the most recently adapted model (fθt) to generate predictions. The current offline protocol, which assumes the stream waits for adaptation, is recovered by setting C(g) = 1 for all methods.",
    "Experimental Setup": "The evaluation was conducted on image classification tasks using a ResNet-50-BN3 backbone (pretrained on ImageNet), with additional experiments on ViT and ResNet-18 architectures. The primary datasets used were ImageNet-C (with 15 corruptions at level 5 severity), ImageNet-R, and ImageNet-3DCC. CIFAR10-C was used for Practical TTA evaluation. The study benchmarked 15 state-of-the-art TTA methods, including BN-adaptation, AdaBN, SHOT, TENT, SAR, EATA, CoTTA, MEMO, DDA, and RoTTA. Experiments were run across four main scenarios: episodic domain shifts, continual domain shifts, varying stream speeds (ηr where η ∈ {1/16, 1/8, 1/4, 1/2, 1}), and practical TTA with label imbalances. Results were reported as average error rates across corruptions and averaged over three random seeds. Batch size was typically 64, with an ablation study on sizes {1, 16, 32, 128}.",
    "Limitations": "The core limitation identified by the paper is that existing TTA methods often incur significantly increased computational costs, which negatively impacts their inference speed and, consequently, their real-world applicability. Current evaluation protocols are criticized for overlooking this extra computation, thereby favoring slower, more complex methods. Specific methods like MEMO and DDA were found to be 'extremely inefficient,' leading to performance close to the non-adapted baseline under realistic conditions. The paper also noted that methods without sample rejection (e.g., SHOT, TENT, TTAC) can overfit to early distributions in offline-continual setups.",
    "Future Research Directions": "The paper explicitly calls for future research to focus on developing TTA methods that are both accurate and computationally efficient, with minimal impact on inference speed. It encourages increasing the efficiency of data-dependent adaptation methods, which were found to be extremely inefficient in the current study. The authors hope their proposed evaluation scheme will inspire TTA methods to consider inference speed as a critical dimension for real-world performance, and highlight the need for efficient TTA methods with negligible additional computational overhead.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
    "Main Contributions": "The research introduces 'recurring TTA,' a novel diagnostic setting where environments not only change but also recur over time, creating an extensive data stream to diagnose the overlooked gradual performance degradation of Test-Time Adaptation (TTA) models. It formally defines TTA model collapse and provides a theoretical analysis on an ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC), shedding light on dataset- and algorithm-dependent factors contributing to error accumulation. The paper proposes 'Persistent TTA' (PeTTA), an adaptation scheme that senses model divergence towards collapse and adaptively adjusts the adaptation strategy, demonstrating supreme stability and persisting performance over existing approaches in lifelong TTA scenarios.",
    "Methodology": "The study conducts a theoretical analysis on an ϵ-GMMC to simulate model collapse, deriving insights into error accumulation factors (prior data distribution, category difference, update rate, false negative rate). Building on this, PeTTA leverages a mean teacher update mechanism, combined with a category-balanced memory bank and robust batch normalization. Its core innovation is a divergence sensing mechanism that measures the Mahalanobis distance of feature embedding vectors from the initial source distribution. This divergence measure (`¯γt`) is used to adaptively adjust the regularization coefficient (`λt = ¯γt · λ0`) and the exponential moving average update rate (`αt = (1 − ¯γt) · α0`). Additionally, an Anchor Loss (LAL), which minimizes the KL divergence between current and source model predictions, is integrated to prevent significant deviation from the initial model.",
    "Experimental Setup": "The theoretical findings are empirically validated through a simulation on an ϵ-GMMC using a synthesized dataset of 6000 samples from two Gaussian distributions. For benchmark experiments, four TTA classification tasks are used: CIFAR-10→CIFAR-10-C, CIFAR-100→CIFAR-100-C, ImageNet→ImageNet-C (all with corruption level 5), and DomainNet (real→clipart, painting, sketch). The proposed 'recurring TTA' setting involves revisiting sequences of changing distributions K=20 times, with temporally correlated batch category distributions generated by Dirichlet distributions. Comparative evaluations are performed against CoTTA, EATA, RMT, MECTA, RoTTA, ROID, TRIBE, LAME, and RDumb. Models are implemented using PyTorch on pre-trained source models from RobustBench and torchvision. PeTTA's hyperparameters (e.g., EMA rate 5e-2, initial α0=1e-3, λ0=10 or 1) are detailed, along with an ablation study on its components and sensitivity analysis for λ0. Additional experiments include evaluation on the Continuously Changing Corruption (CCC) benchmark for prolonged testing streams.",
    "Limitations": "A complete elimination of error accumulation cannot be rigorously guaranteed solely through regularization. PeTTA requires the use of a small memory bank to handle temporally correlated testing streams, and it assumes the availability of pre-computed empirical mean and covariance matrix of feature vectors from the source distribution. These requirements could potentially limit its scalability in certain real-world scenarios. Furthermore, selecting an optimal reset frequency for reset-based methods like RDumb is practically challenging without a validation set, a general TTA limitation.",
    "Future Research Directions": "Future work could focus on developing algorithms that achieve error accumulation-free adaptation by construction, going beyond regularization-based prevention. Exploring alternative methods for reducing memory requirements, such as storing embedded features instead of raw images in the memory bank, is another potential direction. Additionally, research into robustly approximating source data statistics or developing accuracy prediction algorithms for hyperparameter tuning in the absence of labeled validation data would enhance practical applicability.",
    "Experiment Code": "from copy import deepcopy\nimport torch\nimport torch.nn as nn\nimport logging\n\n\nclass BaseAdapter(nn.Module):\n    def __init__(self, cfg, model, optimizer):\n        super().__init__()\n        self.logger = logging.getLogger(\"TTA.adapter\")\n        self.cfg = cfg\n        self.model = self.configure_model(model)\n\n        params, param_names = self.collect_params(self.model)\n        if len(param_names) == 0:\n            self.optimizer = None\n        else:\n            self.optimizer = optimizer(params)\n\n        self.steps = self.cfg.OPTIM.STEPS\n        assert self.steps > 0, \"requires >= 1 step(s) to forward and update\"\n\n    def forward(self, x):\n        for _ in range(self.steps):\n            outputs = self.forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def forward_and_adapt(self, *args):\n        raise NotImplementedError(\"implement forward_and_adapt by yourself!\")\n\n    def configure_model(self, model):\n        raise NotImplementedError(\"implement configure_model by yourself!\")\n\n    def collect_params(self, model: nn.Module):\n        names = []\n        params = []\n\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                names.append(n)\n                params.append(p)\n\n        return params, names\n\n    def check_model(self, model):\n        pass\n\n    def before_tta(self, *args, **kwargs):\n        pass\n\n    @staticmethod\n    def build_ema(model):\n        ema_model = deepcopy(model)\n        for param in ema_model.parameters():\n            param.detach_()\n        return ema_model\n\n\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\nimport torch\nimport torch.nn as nn\nfrom ..utils import memory\nfrom .base_adapter import BaseAdapter\nfrom copy import deepcopy\nfrom .base_adapter import softmax_entropy\nfrom ..utils.bn_layers import RobustBN1d, RobustBN2d\nfrom ..utils.utils import set_named_submodule, get_named_submodule\nfrom ..utils.custom_transforms import get_tta_transforms\n\n\nclass RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        self.mem = memory.CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        self.model_ema = self.build_ema(self.model)\n        self.transform = get_tta_transforms(cfg)\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        # batch data\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # add into memory\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train()\n        self.model_ema.train()\n        # get memory data\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n\n        model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer,\n                                self.cfg.ADAPTER.RoTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\n\nimport random\nimport copy\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\n\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n\nimport torch\nimport torch.nn as nn\nfrom copy import deepcopy\n\n\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\nimport torch\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import ColorJitter, Compose, Lambda\nfrom numpy import random\nimport PIL\nimport torchvision.transforms as transforms\n\n\ndef get_tta_transforms(cfg, gaussian_std: float=0.005, soft=False):\n    img_shape = (*cfg.INPUT.SIZE, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0),\n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),\n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(min_val={0}, max_val={1})'.format(self.min_val, self.max_val)\n\n\nclass ColorJitterPro(ColorJitter):\n    \"\"\"Randomly change the brightness, contrast, saturation, and gamma correction of an image.\"\"\"\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    @staticmethod\n    @torch.jit.unused\n    def get_params(brightness, contrast, saturation, hue, gamma):\n        \"\"\"Get a randomized transform to be applied on image.\n\n        Arguments are same as that of __init__.\n\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        \"\"\"\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n\n        if gamma is not None:\n            gamma_factor = random.uniform(gamma[0], gamma[1])\n            transforms.append(Lambda(lambda img: F.adjust_gamma(img, gamma_factor)))\n\n        random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def forward(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image or Tensor): Input image.\n\n        Returns:\n            PIL Image or Tensor: Color jittered image.\n        \"\"\"\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = F.adjust_brightness(img, brightness_factor)\n\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = F.adjust_contrast(img, contrast_factor)\n\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = F.adjust_saturation(img, saturation_factor)\n\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = F.adjust_hue(img, hue_factor)\n\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0)\n                img = F.adjust_gamma(img, gamma_factor)\n\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += 'brightness={0}'.format(self.brightness)\n        format_string += ', contrast={0}'.format(self.contrast)\n        format_string += ', saturation={0}'.format(self.saturation)\n        format_string += ', hue={0})'.format(self.hue)\n        format_string += ', gamma={0})'.format(self.gamma)\n        return format_string",
    "Experiment Result": "The implemented method, named RoTTA, leverages several mechanisms for test-time adaptation:\n\n1.  **Mean Teacher Update:** An Exponential Moving Average (EMA) model (`model_ema`) is maintained and updated based on the student model (`model`) using a fixed update rate (`nu`). The update rule is `ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]`.\n2.  **Category-Balanced Memory Bank:** A `CSTU` (Category-balanced Spatio-Temporal Uncertainty) memory bank is used to store instances. It has a `MEMORY_SIZE` capacity and maintains balance across classes. Instances are added to the memory, and older/less uncertain instances are removed based on a `heuristic_score` which considers both the instance's age (timeliness, weighted by `LAMBDA_T`) and its uncertainty (entropy, weighted by `LAMBDA_U`).\n3.  **Robust Batch Normalization:** Standard `nn.BatchNorm1d` and `nn.BatchNorm2d` layers in the model are replaced with `RobustBN1d` and `RobustBN2d` respectively. These robust BN layers update their running mean and variance during training (test-time adaptation) using a fixed momentum (`ALPHA`).\n4.  **Anchor Loss (LAL):** A supervised loss (`l_sup`) is computed using memory instances. This loss is a KL divergence (`softmax_entropy`) between the current model's predictions (`stu_sup_out`) on strongly augmented memory data and the EMA model's predictions (`ema_sup_out`) on the original memory data. The loss is weighted by `timeliness_reweighting(ages)`, which gives higher weights to more recent samples.\n5.  **Divergence Sensing (Missing Feature):** The core innovation described in the method, a divergence sensing mechanism using Mahalanobis distance to adaptively adjust the regularization coefficient (`λt`) and EMA update rate (`αt`), is *not found* in the provided RoTTA implementation. The EMA update rate (`nu`) and memory parameters (`LAMBDA_T`, `LAMBDA_U`, `ALPHA`) are fixed by configuration parameters.\n\n**Experimental Settings:**\n*   **Adapter Name:** `RoTTA`\n*   **Memory Bank:**\n    *   `MEMORY_SIZE`: 64\n    *   `LAMBDA_T`: 1.0 (weight for age in heuristic score)\n    *   `LAMBDA_U`: 1.0 (weight for uncertainty in heuristic score)\n*   **EMA Update Rate (`nu`):** 0.001\n*   **Robust BN Momentum (`ALPHA`):** 0.05\n*   **Model Update Frequency:** `UPDATE_FREQUENCY` = 64 (model is updated every 64 instances added to memory).\n*   **Optimizer:** `Adam` with a learning rate (`LR`) of 1e-3.\n*   **Loss Function:** KL divergence between student and EMA model predictions, with timeliness reweighting for memory samples.\n*   **Data Augmentation:** Test-time augmentation pipeline (`get_tta_transforms`) includes `Clip`, `ColorJitterPro` (brightness, contrast, saturation, hue, gamma adjustments), `Pad`, `RandomAffine` (degrees, translate, scale), `GaussianBlur`, `CenterCrop`, `RandomHorizontalFlip`, and `GaussianNoise`."
}
- Identify the most promising existing method that can be improved with minimal modifications to its objective function or core algorithm.
- Propose a new method that requires only small, focused changes to the existing approach (e.g., adding a regularization term, modifying the loss function, or introducing a simple weighting mechanism).
- Ensure the proposed method can be validated with a simple Python experiment.

# Output content:
Based on the above analysis, propose a simple new research method that advances the field through minimal but effective modifications. Your output should include:

- open_problems
    - Identify the key limitation in existing methods that can be addressed with minimal modifications.
    - Focus on problems that can be solved through simple changes to objective functions or algorithms.

- methods
    - Describe the minimal modification to the existing method (e.g., adding regularization, modifying loss function).
    - Explain the theoretical motivation for this change.
    - Keep the modification simple and focused on the identified problem.

- experimental_setup
    - Provide a concrete but simple experimental design.
    - Specify which datasets and evaluation metrics will be used.
    - Design a straightforward comparison with the base method.

- experimental_code
    - Output the core Python code implementing the proposed modification.
    - Focus only on the key changes to the base method.
    - Keep the code concise and readable.

- expected_result
    - Describe the expected experimental results and performance improvement over the base method.

- expected_conclusion
    - Summarize the practical value of the minimal modification.
    - Explain why this simple change leads to meaningful improvement.
Output:
{
    "open_problems": "Fast methods such as TENT already adapt on‐line and touch only the BatchNorm parameters, but they still need dozens of test samples before their error drops to the plateau obtained after long adaptation. The root cause is a fixed, conservative learning-rate that cannot distinguish between (1) an early, highly shifted batch that requires a large corrective step and (2) a late, already-aligned batch where only a tiny refinement is safe. A simple mechanism that automatically takes larger steps when the model is very uncertain and smaller ones when it is confident is still missing.",
    "methods": "Entropy-Scaled TENT (E-TENT).\nBase method: TENT (Wang et al., 2021) – minimises the prediction entropy E(x)=−∑ p log p of each incoming batch with SGD on the affine BN parameters.\nMinimal change: multiply the update step by a scalar s that is computed from the mean entropy of the current batch.\n Let  Ē = 1/B ∑_i E(x_i)  and E_max = log(C) (maximum entropy for C classes).\n Define the scale  s = 1 + κ · ( Ē / E_max )  , clipped to [1 , 1+κ_max].\nImplementation: before optimizer.step(), loop over all gradients and do  g ← s·g  (or equivalently set lr ← s·lr for that step).  κ is a single hyper-parameter (default 1.0) and κ_max is 2.0 to guarantee stability.\nTheoretical intuition:  Ē /E_max  is a proxy for the model’s epistemic uncertainty about the current domain shift.  A larger s accelerates descent on the entropy surface when uncertainty is high, reducing adaptation time; as the model converges and entropy falls, s→1 yielding the original, stable TENT behaviour.  No additional forward/backward passes, parameters, or memory are introduced, so computational cost is unchanged.",
    "experimental_setup": "Backbone: ResNet-50 pretrained on ImageNet.\nDatasets: ImageNet-C (severity 5) streamed at constant speed r as in \"Evaluation of TTA under Computational Time Constraints\".\nBaselines: (i) Source model (no TTA) (ii) Original TENT with identical base learning rate 1e-3.\nMetrics:\n1. Online error rate after each test sample (plot).\n2. Samples-to-plateau: number of seen images required to reach within 1% of final TENT accuracy.\n3. Area-Under-Error-Curve (AUEC) – lower is faster convergence.\nHyper-parameters: κ∈{0.5,1,1.5}; κ_max=2.0; same batch size 64.\nCompute budget: identical to TENT (one forward+backward per batch).",
    "experimental_code": "# core difference with TENT in less than 30 lines\nclass ETENT(TENT):\n    \"\"\"Entropy-Scaled TENT: inherits all from the reference TENT implementation\"\"\"\n    def __init__(self, model, lr=1e-3, kappa=1.0, kappa_max=2.0):\n        super().__init__(model, lr)\n        self.kappa = kappa\n        self.kappa_max = kappa_max\n        self.c = model.fc.out_features  # num classes, works for common nets\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, x):\n        # identical to TENT up to loss computation\n        outputs = self.model(x)\n        entropy = softmax_entropy(outputs, outputs).mean()\n        entropy.backward()\n\n        # --- new: scale gradients by current mean entropy --- #\n        with torch.no_grad():\n            e_hat = entropy.item() / math.log(self.c)          # normalised [0,1]\n            s = 1.0 + self.kappa * e_hat\n            s = min(s, 1.0 + self.kappa_max)\n            for p in self.model.parameters():\n                if p.grad is not None:\n                    p.grad.mul_(s)\n        # ----------------------------------------------- #\n\n        self.optimizer.step()\n        self.optimizer.zero_grad(set_to_none=True)\n        return outputs",
    "expected_result": "1. ETENT reaches the TENT plateau error (≈24% on ImageNet-C, ResNet-50) after ~3× fewer samples (e.g., 2,000 vs 6,000).\n2. AUEC is reduced by 15-20% across corruptions, indicating faster adaptation.\n3. Final error after full stream is identical (+/-0.1%) to TENT, showing no loss of stability.\n4. Wall-clock time per batch is unchanged (∼0.9 ms extra for scaling factor).",
    "expected_conclusion": "A single line rescaling of the gradient by the current batch entropy lets the model take large corrective steps exactly when they are needed and automatically anneals the step size as the model becomes confident, giving much faster convergence without extra computation, parameters, or complex scheduling.  This demonstrates that carefully leveraging an already computed quantity (prediction entropy) can yield meaningful practical gains for Test-Time Adaptation with almost zero implementation overhead."
}
